2025-12-25 08:33:26,562 - INFO - root - Streamlit UI Loaded
2025-12-25 08:34:19,410 - INFO - root - Streamlit UI Loaded
2025-12-25 08:34:19,621 - INFO - root - Streamlit UI Loaded
2025-12-25 08:34:19,635 - INFO - root - User clicked Generate Test Cases
2025-12-25 08:34:19,636 - INFO - src.security - Starting requirement validation process
2025-12-25 08:34:19,638 - INFO - src.security - Requirement validation successful
2025-12-25 08:34:19,638 - INFO - root - Requirement validation successful
2025-12-25 08:34:19,641 - INFO - src.generator - Preparing test case generation request
2025-12-25 08:34:19,641 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3
2025-12-25 08:34:19,642 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 08:36:19,754 - ERROR - src.ollama_client - Ollama process timed out
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 53, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=120  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3', '\nYou are a senior QA Architect.\nGenerate COMPLETE professional test cases.\n\nRequirement:\nSystem should process online payments using debit/credit card. If payment fails, system should show failure status and not confirm order.\n\nStrict Output Format (MANDATORY):\n------------------------------------\nFunctional Test Cases:\n1)\n2)\n\nBoundary Test Cases:\n1)\n2)\n\nNegative Test Cases:\n1)\n2)\n\nEdge Cases:\n1)\n2)\n\nAcceptance Criteria:\n1)\n2)\n------------------------------------\nRules:\n- Do NOT add extra explanation text\n- Only produce the structured sections above\n- Number items properly\n']' timed out after 120 seconds
2025-12-25 08:36:19,757 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 53, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=120  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3', '\nYou are a senior QA Architect.\nGenerate COMPLETE professional test cases.\n\nRequirement:\nSystem should process online payments using debit/credit card. If payment fails, system should show failure status and not confirm order.\n\nStrict Output Format (MANDATORY):\n------------------------------------\nFunctional Test Cases:\n1)\n2)\n\nBoundary Test Cases:\n1)\n2)\n\nNegative Test Cases:\n1)\n2)\n\nEdge Cases:\n1)\n2)\n\nAcceptance Criteria:\n1)\n2)\n------------------------------------\nRules:\n- Do NOT add extra explanation text\n- Only produce the structured sections above\n- Number items properly\n']' timed out after 120 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 62, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 78, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out
2025-12-25 08:36:19,763 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 53, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=120  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3', '\nYou are a senior QA Architect.\nGenerate COMPLETE professional test cases.\n\nRequirement:\nSystem should process online payments using debit/credit card. If payment fails, system should show failure status and not confirm order.\n\nStrict Output Format (MANDATORY):\n------------------------------------\nFunctional Test Cases:\n1)\n2)\n\nBoundary Test Cases:\n1)\n2)\n\nNegative Test Cases:\n1)\n2)\n\nEdge Cases:\n1)\n2)\n\nAcceptance Criteria:\n1)\n2)\n------------------------------------\nRules:\n- Do NOT add extra explanation text\n- Only produce the structured sections above\n- Number items properly\n']' timed out after 120 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 62, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 78, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 86, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 08:38:05,949 - INFO - root - Streamlit UI Loaded
2025-12-25 08:38:05,959 - INFO - root - User clicked Generate Test Cases
2025-12-25 08:38:05,959 - INFO - src.security - Starting requirement validation process
2025-12-25 08:38:05,960 - INFO - src.security - Requirement validation successful
2025-12-25 08:38:05,960 - INFO - root - Requirement validation successful
2025-12-25 08:38:05,962 - INFO - src.generator - Preparing test case generation request
2025-12-25 08:38:05,962 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3
2025-12-25 08:38:05,963 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 08:39:12,348 - INFO - src.ollama_client - AI response received successfully
2025-12-25 08:39:12,348 - INFO - src.generator - Test case generation completed successfully
2025-12-25 08:39:12,353 - INFO - root - Test Cases delivered successfully to UI
2025-12-25 08:40:09,872 - INFO - root - Streamlit UI Loaded
2025-12-25 08:54:04,317 - INFO - root - Streamlit UI Loaded
2025-12-25 08:55:23,114 - INFO - root - Streamlit UI Loaded
2025-12-25 08:55:23,319 - INFO - root - Streamlit UI Loaded
2025-12-25 08:55:23,327 - INFO - root - User clicked Generate Test Cases
2025-12-25 08:55:23,328 - INFO - src.security - Starting requirement validation process
2025-12-25 08:55:23,329 - INFO - src.security - Requirement validation successful
2025-12-25 08:55:23,329 - INFO - root - Requirement validation successful
2025-12-25 08:55:23,332 - INFO - src.generator - Preparing advanced enterprise test case generation request
2025-12-25 08:55:23,332 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3
2025-12-25 08:55:23,333 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 08:57:48,346 - INFO - root - Streamlit UI Loaded
2025-12-25 08:57:48,355 - INFO - root - User clicked Generate Test Cases
2025-12-25 08:57:48,357 - INFO - src.security - Starting requirement validation process
2025-12-25 08:57:48,358 - INFO - src.security - Requirement validation successful
2025-12-25 08:57:48,358 - INFO - root - Requirement validation successful
2025-12-25 08:57:48,361 - INFO - src.generator - Preparing advanced enterprise test case generation request
2025-12-25 08:57:48,362 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3
2025-12-25 08:57:48,362 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 09:01:27,654 - INFO - root - Streamlit UI Loaded
2025-12-25 09:01:37,917 - INFO - root - Streamlit UI Loaded
2025-12-25 09:01:38,142 - INFO - root - Streamlit UI Loaded
2025-12-25 09:01:38,151 - INFO - root - User clicked Generate Test Cases
2025-12-25 09:01:38,152 - INFO - src.security - Starting requirement validation process
2025-12-25 09:01:38,153 - INFO - src.security - Requirement validation successful
2025-12-25 09:01:38,154 - INFO - root - Requirement validation successful
2025-12-25 09:01:38,157 - INFO - src.generator - Preparing advanced enterprise test case generation request
2025-12-25 09:01:38,159 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3
2025-12-25 09:01:38,160 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 09:05:31,951 - INFO - root - Streamlit UI Loaded
2025-12-25 09:05:43,307 - INFO - root - Streamlit UI Loaded
2025-12-25 09:05:43,526 - INFO - root - Streamlit UI Loaded
2025-12-25 09:05:43,534 - INFO - root - User clicked Generate Test Cases
2025-12-25 09:05:43,536 - INFO - src.security - Starting requirement validation process
2025-12-25 09:05:43,537 - INFO - src.security - Requirement validation successful
2025-12-25 09:05:43,538 - INFO - root - Requirement validation successful
2025-12-25 09:05:43,540 - INFO - src.generator - Preparing advanced enterprise test case generation request
2025-12-25 09:05:43,541 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3
2025-12-25 09:05:43,542 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 09:10:16,716 - ERROR - src.ollama_client - Ollama returned empty response
2025-12-25 09:10:16,717 - ERROR - src.ollama_client - AI processing error
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 71, in generate_ai_response
    raise RuntimeError("Empty response received from AI")
RuntimeError: Empty response received from AI
2025-12-25 09:10:16,718 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 71, in generate_ai_response
    raise RuntimeError("Empty response received from AI")
RuntimeError: Empty response received from AI

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 138, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 82, in generate_ai_response
    logger.exception("AI processing error")
    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: AI processing error
2025-12-25 09:10:16,720 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 71, in generate_ai_response
    raise RuntimeError("Empty response received from AI")
RuntimeError: Empty response received from AI

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 138, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 82, in generate_ai_response
    logger.exception("AI processing error")
    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: AI processing error

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 164, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 09:13:33,749 - INFO - root - Streamlit UI Loaded
2025-12-25 09:13:44,330 - INFO - root - Streamlit UI Loaded
2025-12-25 09:13:44,543 - INFO - root - Streamlit UI Loaded
2025-12-25 09:13:44,554 - INFO - root - User clicked Generate Test Cases
2025-12-25 09:13:44,556 - INFO - src.security - Starting requirement validation process
2025-12-25 09:13:44,557 - INFO - src.security - Requirement validation successful
2025-12-25 09:13:44,558 - INFO - root - Requirement validation successful
2025-12-25 09:13:44,561 - INFO - src.generator - Preparing advanced enterprise test case generation request
2025-12-25 09:13:44,562 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 09:13:44,564 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 09:18:36,697 - ERROR - src.ollama_client - Ollama returned empty response
2025-12-25 09:18:36,699 - ERROR - src.ollama_client - AI processing error
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 71, in generate_ai_response
    raise RuntimeError("Empty response received from AI")
RuntimeError: Empty response received from AI
2025-12-25 09:18:36,703 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 71, in generate_ai_response
    raise RuntimeError("Empty response received from AI")
RuntimeError: Empty response received from AI

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 138, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 83, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error
2025-12-25 09:18:36,708 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 71, in generate_ai_response
    raise RuntimeError("Empty response received from AI")
RuntimeError: Empty response received from AI

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 138, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 83, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 164, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 09:22:45,059 - INFO - root - Streamlit UI Loaded
2025-12-25 09:22:45,275 - INFO - root - Streamlit UI Loaded
2025-12-25 09:22:45,277 - INFO - root - User clicked Generate Test Cases
2025-12-25 09:22:45,277 - INFO - src.security - Starting requirement validation process
2025-12-25 09:22:45,278 - INFO - src.security - Requirement validation successful
2025-12-25 09:22:45,278 - INFO - root - Requirement validation successful
2025-12-25 09:22:45,278 - INFO - src.generator - Preparing advanced enterprise test case generation request
2025-12-25 09:22:45,278 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 09:22:45,279 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 09:26:09,367 - INFO - root - Streamlit UI Loaded
2025-12-25 09:26:09,374 - INFO - root - User clicked Generate Test Cases
2025-12-25 09:26:09,377 - INFO - src.security - Starting requirement validation process
2025-12-25 09:26:09,377 - INFO - src.security - Requirement validation successful
2025-12-25 09:26:09,379 - INFO - root - Requirement validation successful
2025-12-25 09:26:09,381 - INFO - src.generator - Preparing advanced enterprise test case generation request
2025-12-25 09:26:09,382 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 09:26:09,382 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 09:26:16,857 - INFO - src.ollama_client - AI response received successfully
2025-12-25 09:26:16,857 - ERROR - src.ollama_client - Ollama returned empty response
2025-12-25 09:26:16,861 - ERROR - src.ollama_client - AI processing error
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 71, in generate_ai_response
    raise RuntimeError("Empty response received from AI")
RuntimeError: Empty response received from AI
2025-12-25 09:26:16,858 - WARNING - src.generator - Expected key section missing: PERFORMANCE TEST SCENARIOS
2025-12-25 09:26:16,864 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 71, in generate_ai_response
    raise RuntimeError("Empty response received from AI")
RuntimeError: Empty response received from AI

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 138, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 83, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error
2025-12-25 09:26:16,865 - WARNING - src.generator - Expected key section missing: SECURITY TEST SCENARIOS
2025-12-25 09:26:16,870 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 71, in generate_ai_response
    raise RuntimeError("Empty response received from AI")
RuntimeError: Empty response received from AI

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 138, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 83, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 164, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 09:26:16,870 - WARNING - src.generator - Expected key section missing: AUTOMATION FRIENDLY OUTPUT
2025-12-25 09:26:16,912 - WARNING - src.generator - Expected key section missing: DEFECT PREDICTION
2025-12-25 09:26:16,912 - WARNING - src.generator - Expected key section missing: METRICS SUMMARY
2025-12-25 09:26:16,913 - INFO - src.generator - Advanced test case generation completed successfully
2025-12-25 09:26:29,310 - INFO - root - Streamlit UI Loaded
2025-12-25 09:26:37,557 - INFO - root - Streamlit UI Loaded
2025-12-25 09:26:37,767 - INFO - root - Streamlit UI Loaded
2025-12-25 09:26:37,776 - INFO - root - User clicked Generate Test Cases
2025-12-25 09:26:37,777 - INFO - src.security - Starting requirement validation process
2025-12-25 09:26:37,777 - INFO - src.security - Requirement validation successful
2025-12-25 09:26:37,778 - INFO - root - Requirement validation successful
2025-12-25 09:26:37,780 - INFO - src.generator - Preparing advanced enterprise test case generation request
2025-12-25 09:26:37,781 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 09:26:37,781 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 09:28:37,848 - INFO - src.ollama_client - Ollama process timed out
2025-12-25 09:36:11,550 - INFO - root - Streamlit UI Loaded
2025-12-25 09:36:17,072 - INFO - root - Streamlit UI Loaded
2025-12-25 09:36:17,316 - INFO - root - Streamlit UI Loaded
2025-12-25 09:36:17,320 - INFO - root - User clicked Generate Test Cases
2025-12-25 09:36:17,320 - INFO - src.security - Starting requirement validation process
2025-12-25 09:36:17,321 - INFO - src.security - Requirement validation successful
2025-12-25 09:36:17,321 - INFO - root - Requirement validation successful
2025-12-25 09:36:17,322 - INFO - src.generator - Preparing advanced enterprise test case generation request
2025-12-25 09:36:17,323 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 09:36:17,323 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 09:38:09,049 - INFO - root - Streamlit UI Loaded
2025-12-25 09:38:16,059 - INFO - root - Streamlit UI Loaded
2025-12-25 09:38:16,273 - INFO - root - Streamlit UI Loaded
2025-12-25 09:38:16,280 - INFO - root - User clicked Generate Test Cases
2025-12-25 09:38:16,281 - INFO - src.security - Starting requirement validation process
2025-12-25 09:38:16,281 - INFO - src.security - Requirement validation successful
2025-12-25 09:38:16,282 - INFO - root - Requirement validation successful
2025-12-25 09:38:16,283 - INFO - src.generator - Preparing advanced enterprise test case generation request
2025-12-25 09:38:16,284 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 09:38:16,284 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 09:38:17,384 - INFO - src.ollama_client - Ollama process timed out
2025-12-25 09:40:16,333 - INFO - src.ollama_client - Ollama process timed out
2025-12-25 09:42:49,626 - INFO - root - Streamlit UI Loaded
2025-12-25 09:42:58,953 - INFO - root - Streamlit UI Loaded
2025-12-25 09:43:01,914 - INFO - root - Streamlit UI Loaded
2025-12-25 09:43:01,923 - INFO - root - User clicked Generate Test Cases
2025-12-25 09:43:01,924 - INFO - src.security - Starting requirement validation process
2025-12-25 09:43:01,925 - INFO - src.security - Requirement validation successful
2025-12-25 09:43:01,925 - INFO - root - Requirement validation successful
2025-12-25 09:43:01,927 - INFO - src.generator - Preparing optimized test case generation request
2025-12-25 09:43:01,928 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 09:43:01,928 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 09:45:02,004 - INFO - src.ollama_client - Ollama process timed out
2025-12-25 09:45:44,393 - INFO - root - Streamlit UI Loaded
2025-12-25 09:45:48,964 - INFO - root - Streamlit UI Loaded
2025-12-25 09:45:49,191 - INFO - root - Streamlit UI Loaded
2025-12-25 09:45:49,202 - INFO - root - User clicked Generate Test Cases
2025-12-25 09:45:49,204 - INFO - src.security - Starting requirement validation process
2025-12-25 09:45:49,205 - INFO - src.security - Requirement validation successful
2025-12-25 09:45:49,206 - INFO - root - Requirement validation successful
2025-12-25 09:45:49,208 - INFO - src.generator - Preparing optimized test case generation request
2025-12-25 09:45:49,209 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 09:45:49,210 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 09:47:35,003 - INFO - root - Streamlit UI Loaded
2025-12-25 09:47:38,801 - INFO - root - Streamlit UI Loaded
2025-12-25 09:47:38,812 - INFO - root - User clicked Generate Test Cases
2025-12-25 09:47:38,813 - INFO - src.security - Starting requirement validation process
2025-12-25 09:47:38,815 - INFO - src.security - Requirement validation successful
2025-12-25 09:47:38,816 - INFO - root - Requirement validation successful
2025-12-25 09:47:38,821 - INFO - src.generator - Preparing optimized test case generation request
2025-12-25 09:47:38,822 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 09:47:38,824 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 09:47:49,275 - INFO - src.ollama_client - Ollama process timed out
2025-12-25 09:49:16,949 - INFO - root - Streamlit UI Loaded
2025-12-25 09:49:38,932 - INFO - src.ollama_client - Ollama process timed out
2025-12-25 09:54:30,113 - INFO - root - Streamlit UI Loaded
2025-12-25 09:54:46,339 - INFO - root - Streamlit UI Loaded
2025-12-25 09:54:47,193 - INFO - root - Streamlit UI Loaded
2025-12-25 09:54:47,203 - INFO - root - User clicked Generate Test Cases
2025-12-25 09:54:47,204 - INFO - src.security - Starting requirement validation process
2025-12-25 09:54:47,205 - INFO - src.security - Requirement validation successful
2025-12-25 09:54:47,205 - INFO - root - Requirement validation successful
2025-12-25 09:54:47,209 - INFO - src.generator - Preparing optimized test case generation request
2025-12-25 09:54:47,210 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 09:54:47,211 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 09:56:47,245 - ERROR - src.ollama_client - Ollama process timed out
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 53, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=120  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nYou are a senior QA Test Engineer.\nGenerate a clean MANUAL TEST CASE SUITE.\n\nRequirement:\nUser should be able to login with username and password. \nAccount should lock after 5 failed login attempts.\n\n\nReturn ONLY:\n- 4 Functional Scenarios\n- 2 Boundary Scenarios\n- 2 Negative Scenarios\n- 1 Edge Case Scenario\n- Acceptance Criteria\n- Short Test Summary\n\nSTRICT FORMAT (MANDATORY):\n------------------------------------\nSCENARIO NAME:\nOBJECTIVE:\n\nTEST CASE ID:\nTITLE:\nPRIORITY:\nPRE CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\n------------------------------------\nRepeat for each scenario.\nDo NOT add extra explanation text.\nDo NOT add matrix.\nDo NOT add automation.\nDo NOT add performance/security.\nONLY this structured test case content.\n']' timed out after 120 seconds
2025-12-25 09:56:47,248 - INFO - src.ollama_client - Ollama process timed out
2025-12-25 09:56:47,248 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 53, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=120  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nYou are a senior QA Test Engineer.\nGenerate a clean MANUAL TEST CASE SUITE.\n\nRequirement:\nUser should be able to login with username and password. \nAccount should lock after 5 failed login attempts.\n\n\nReturn ONLY:\n- 4 Functional Scenarios\n- 2 Boundary Scenarios\n- 2 Negative Scenarios\n- 1 Edge Case Scenario\n- Acceptance Criteria\n- Short Test Summary\n\nSTRICT FORMAT (MANDATORY):\n------------------------------------\nSCENARIO NAME:\nOBJECTIVE:\n\nTEST CASE ID:\nTITLE:\nPRIORITY:\nPRE CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\n------------------------------------\nRepeat for each scenario.\nDo NOT add extra explanation text.\nDo NOT add matrix.\nDo NOT add automation.\nDo NOT add performance/security.\nONLY this structured test case content.\n']' timed out after 120 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 70, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 79, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out
2025-12-25 09:56:47,251 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 53, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=120  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nYou are a senior QA Test Engineer.\nGenerate a clean MANUAL TEST CASE SUITE.\n\nRequirement:\nUser should be able to login with username and password. \nAccount should lock after 5 failed login attempts.\n\n\nReturn ONLY:\n- 4 Functional Scenarios\n- 2 Boundary Scenarios\n- 2 Negative Scenarios\n- 1 Edge Case Scenario\n- Acceptance Criteria\n- Short Test Summary\n\nSTRICT FORMAT (MANDATORY):\n------------------------------------\nSCENARIO NAME:\nOBJECTIVE:\n\nTEST CASE ID:\nTITLE:\nPRIORITY:\nPRE CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\n------------------------------------\nRepeat for each scenario.\nDo NOT add extra explanation text.\nDo NOT add matrix.\nDo NOT add automation.\nDo NOT add performance/security.\nONLY this structured test case content.\n']' timed out after 120 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 70, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 79, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 93, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 09:57:48,751 - INFO - root - Streamlit UI Loaded
2025-12-25 09:57:53,925 - INFO - root - Streamlit UI Loaded
2025-12-25 09:57:54,163 - INFO - root - Streamlit UI Loaded
2025-12-25 09:57:54,166 - INFO - root - User clicked Generate Test Cases
2025-12-25 09:57:54,167 - INFO - src.security - Starting requirement validation process
2025-12-25 09:57:54,167 - INFO - src.security - Requirement validation successful
2025-12-25 09:57:54,167 - INFO - root - Requirement validation successful
2025-12-25 09:57:54,168 - INFO - src.generator - Preparing optimized test case generation request
2025-12-25 09:57:54,169 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 09:57:54,169 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 09:58:04,360 - INFO - src.ollama_client - AI response received successfully
2025-12-25 09:58:04,361 - WARNING - src.generator - Expected key section missing: REQUIREMENT TO TEST CASE TRACEABILITY MATRIX
2025-12-25 09:58:04,362 - WARNING - src.generator - Expected key section missing: DETAILED TEST SCENARIOS
2025-12-25 09:58:04,362 - WARNING - src.generator - Expected key section missing: AUTOMATION FRIENDLY OUTPUT
2025-12-25 09:58:04,363 - WARNING - src.generator - Expected key section missing: METRICS SUMMARY
2025-12-25 09:58:04,363 - INFO - src.generator - Optimized test case generation completed successfully
2025-12-25 09:58:04,368 - INFO - root - Test Cases delivered successfully to UI
2025-12-25 10:01:03,483 - INFO - root - Streamlit UI Loaded
2025-12-25 10:01:10,788 - INFO - root - Streamlit UI Loaded
2025-12-25 10:01:11,026 - INFO - root - Streamlit UI Loaded
2025-12-25 10:01:11,037 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:01:11,038 - INFO - src.security - Starting requirement validation process
2025-12-25 10:01:11,038 - INFO - src.security - Requirement validation successful
2025-12-25 10:01:11,039 - INFO - root - Requirement validation successful
2025-12-25 10:01:11,041 - INFO - src.generator - Preparing optimized test case generation request
2025-12-25 10:01:11,042 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:01:11,043 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 10:01:36,039 - INFO - src.ollama_client - AI response received successfully
2025-12-25 10:01:36,040 - WARNING - src.generator - Expected key section missing: REQUIREMENT TO TEST CASE TRACEABILITY MATRIX
2025-12-25 10:01:36,040 - WARNING - src.generator - Expected key section missing: DETAILED TEST SCENARIOS
2025-12-25 10:01:36,041 - WARNING - src.generator - Expected key section missing: AUTOMATION FRIENDLY OUTPUT
2025-12-25 10:01:36,041 - WARNING - src.generator - Expected key section missing: METRICS SUMMARY
2025-12-25 10:01:36,041 - INFO - src.generator - Optimized test case generation completed successfully
2025-12-25 10:01:36,044 - INFO - root - Test Cases delivered successfully to UI
2025-12-25 10:03:38,256 - INFO - root - Streamlit UI Loaded
2025-12-25 10:03:59,754 - INFO - root - Streamlit UI Loaded
2025-12-25 10:03:59,983 - INFO - root - Streamlit UI Loaded
2025-12-25 10:03:59,988 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:03:59,988 - INFO - src.security - Starting requirement validation process
2025-12-25 10:03:59,989 - INFO - src.security - Requirement validation successful
2025-12-25 10:03:59,989 - INFO - root - Requirement validation successful
2025-12-25 10:03:59,990 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 10:03:59,990 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:03:59,990 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 10:06:00,079 - ERROR - src.ollama_client - Ollama process timed out
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 53, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=120  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nUser should be able to login with username and password. \nAccount must lock after 5 failed attempts.\nSession should timeout after 15 minutes.\n\n\nYou are NOT a chatbot.\nDO NOT greet.\nDO NOT explain anything.\nDO NOT add any extra text.\n\nReturn ONLY structured manual QA test cases in this exact format:\n\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nPRE CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\nGenerate:\n- 4 Functional Scenarios\n- 2 Negative Scenarios\n- 1 Boundary Scenario\n- 1 Edge Case Scenario\n- Acceptance Criteria Section\n- Short Test Summary Section\n\nSTRICT RULES:\n- No greetings\n- No emojis\n- No narrative text\n- Only structured test cases\n']' timed out after 120 seconds
2025-12-25 10:06:00,090 - INFO - src.ollama_client - Ollama process timed out
2025-12-25 10:06:00,090 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 53, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=120  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nUser should be able to login with username and password. \nAccount must lock after 5 failed attempts.\nSession should timeout after 15 minutes.\n\n\nYou are NOT a chatbot.\nDO NOT greet.\nDO NOT explain anything.\nDO NOT add any extra text.\n\nReturn ONLY structured manual QA test cases in this exact format:\n\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nPRE CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\nGenerate:\n- 4 Functional Scenarios\n- 2 Negative Scenarios\n- 1 Boundary Scenario\n- 1 Edge Case Scenario\n- Acceptance Criteria Section\n- Short Test Summary Section\n\nSTRICT RULES:\n- No greetings\n- No emojis\n- No narrative text\n- Only structured test cases\n']' timed out after 120 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 79, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out
2025-12-25 10:06:00,097 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 53, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=120  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nUser should be able to login with username and password. \nAccount must lock after 5 failed attempts.\nSession should timeout after 15 minutes.\n\n\nYou are NOT a chatbot.\nDO NOT greet.\nDO NOT explain anything.\nDO NOT add any extra text.\n\nReturn ONLY structured manual QA test cases in this exact format:\n\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nPRE CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\nGenerate:\n- 4 Functional Scenarios\n- 2 Negative Scenarios\n- 1 Boundary Scenario\n- 1 Edge Case Scenario\n- Acceptance Criteria Section\n- Short Test Summary Section\n\nSTRICT RULES:\n- No greetings\n- No emojis\n- No narrative text\n- Only structured test cases\n']' timed out after 120 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 79, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 112, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 10:08:58,624 - INFO - root - Streamlit UI Loaded
2025-12-25 10:09:03,868 - INFO - root - Streamlit UI Loaded
2025-12-25 10:09:04,094 - INFO - root - Streamlit UI Loaded
2025-12-25 10:09:04,097 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:09:04,097 - INFO - src.security - Starting requirement validation process
2025-12-25 10:09:04,097 - INFO - src.security - Requirement validation successful
2025-12-25 10:09:04,097 - INFO - root - Requirement validation successful
2025-12-25 10:09:04,098 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 10:09:04,099 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:09:04,099 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 10:11:04,125 - ERROR - src.ollama_client - Ollama process timed out
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 53, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=120  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nhi\n\nYou are NOT a chatbot.\nDO NOT greet.\nDO NOT explain anything.\nDO NOT add any extra text.\n\nReturn ONLY structured manual QA test cases in this exact format:\n\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nPRE CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\nGenerate:\n- 4 Functional Scenarios\n- 2 Negative Scenarios\n- 1 Boundary Scenario\n- 1 Edge Case Scenario\n- Acceptance Criteria Section\n- Short Test Summary Section\n\nSTRICT RULES:\n- No greetings\n- No emojis\n- No narrative text\n- Only structured test cases\n']' timed out after 120 seconds
2025-12-25 10:11:04,127 - INFO - src.ollama_client - Ollama process timed out
2025-12-25 10:11:04,128 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 53, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=120  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nhi\n\nYou are NOT a chatbot.\nDO NOT greet.\nDO NOT explain anything.\nDO NOT add any extra text.\n\nReturn ONLY structured manual QA test cases in this exact format:\n\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nPRE CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\nGenerate:\n- 4 Functional Scenarios\n- 2 Negative Scenarios\n- 1 Boundary Scenario\n- 1 Edge Case Scenario\n- Acceptance Criteria Section\n- Short Test Summary Section\n\nSTRICT RULES:\n- No greetings\n- No emojis\n- No narrative text\n- Only structured test cases\n']' timed out after 120 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 79, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out
2025-12-25 10:11:04,131 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 53, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=120  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nhi\n\nYou are NOT a chatbot.\nDO NOT greet.\nDO NOT explain anything.\nDO NOT add any extra text.\n\nReturn ONLY structured manual QA test cases in this exact format:\n\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nPRE CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\nGenerate:\n- 4 Functional Scenarios\n- 2 Negative Scenarios\n- 1 Boundary Scenario\n- 1 Edge Case Scenario\n- Acceptance Criteria Section\n- Short Test Summary Section\n\nSTRICT RULES:\n- No greetings\n- No emojis\n- No narrative text\n- Only structured test cases\n']' timed out after 120 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 79, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 112, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 10:14:57,049 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:01,412 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:01,642 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:01,648 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:15:01,648 - INFO - src.security - Starting requirement validation process
2025-12-25 10:15:01,649 - INFO - src.security - Requirement validation successful
2025-12-25 10:15:01,649 - INFO - root - Requirement validation successful
2025-12-25 10:15:01,651 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 10:15:01,652 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:15:01,652 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 10:15:01,715 - ERROR - src.ollama_client - Ollama execution failed | Return Code: 1 | Error: Error: unknown flag: --num-predict

2025-12-25 10:15:01,715 - ERROR - src.ollama_client - AI processing error
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed
2025-12-25 10:15:01,716 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 85, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error
2025-12-25 10:15:01,717 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 85, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 112, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 10:15:05,431 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:05,434 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:15:05,434 - INFO - src.security - Starting requirement validation process
2025-12-25 10:15:05,435 - INFO - src.security - Requirement validation successful
2025-12-25 10:15:05,435 - INFO - root - Requirement validation successful
2025-12-25 10:15:05,435 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 10:15:05,436 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:15:05,436 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 10:15:05,479 - ERROR - src.ollama_client - Ollama execution failed | Return Code: 1 | Error: Error: unknown flag: --num-predict

2025-12-25 10:15:05,479 - ERROR - src.ollama_client - AI processing error
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed
2025-12-25 10:15:05,479 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 85, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error
2025-12-25 10:15:05,480 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 85, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 112, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 10:15:07,554 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:07,559 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:15:07,559 - INFO - src.security - Starting requirement validation process
2025-12-25 10:15:07,560 - INFO - src.security - Requirement validation successful
2025-12-25 10:15:07,560 - INFO - root - Requirement validation successful
2025-12-25 10:15:07,561 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 10:15:07,562 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:15:07,562 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 10:15:07,624 - ERROR - src.ollama_client - Ollama execution failed | Return Code: 1 | Error: Error: unknown flag: --num-predict

2025-12-25 10:15:07,624 - ERROR - src.ollama_client - AI processing error
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed
2025-12-25 10:15:07,625 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 85, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error
2025-12-25 10:15:07,626 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 85, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 112, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 10:15:18,666 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:18,670 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:15:18,670 - INFO - src.security - Starting requirement validation process
2025-12-25 10:15:18,671 - INFO - src.security - Requirement validation successful
2025-12-25 10:15:18,671 - INFO - root - Requirement validation successful
2025-12-25 10:15:18,673 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 10:15:18,673 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:15:18,673 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 10:15:18,718 - ERROR - src.ollama_client - Ollama execution failed | Return Code: 1 | Error: Error: unknown flag: --num-predict

2025-12-25 10:15:18,718 - ERROR - src.ollama_client - AI processing error
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed
2025-12-25 10:15:18,719 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 85, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error
2025-12-25 10:15:18,720 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 85, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 112, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 10:15:19,682 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:23,271 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:23,509 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:23,513 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:15:23,513 - INFO - src.security - Starting requirement validation process
2025-12-25 10:15:23,514 - INFO - src.security - Requirement validation successful
2025-12-25 10:15:23,514 - INFO - root - Requirement validation successful
2025-12-25 10:15:23,515 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 10:15:23,515 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:15:23,515 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 10:15:23,560 - ERROR - src.ollama_client - Ollama execution failed | Return Code: 1 | Error: Error: unknown flag: --num-predict

2025-12-25 10:15:23,560 - ERROR - src.ollama_client - AI processing error
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed
2025-12-25 10:15:23,561 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 85, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error
2025-12-25 10:15:23,562 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 85, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 112, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 10:15:37,757 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:41,446 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:41,676 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:41,680 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:15:41,680 - INFO - src.security - Starting requirement validation process
2025-12-25 10:15:41,680 - INFO - src.security - Requirement validation successful
2025-12-25 10:15:41,680 - INFO - root - Requirement validation successful
2025-12-25 10:15:41,681 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 10:15:41,682 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:15:41,682 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 10:15:41,725 - ERROR - src.ollama_client - Ollama execution failed | Return Code: 1 | Error: Error: unknown flag: --num-predict

2025-12-25 10:15:41,725 - ERROR - src.ollama_client - AI processing error
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed
2025-12-25 10:15:41,726 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 85, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error
2025-12-25 10:15:41,727 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 85, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 112, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 10:15:49,003 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:54,553 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:54,777 - INFO - root - Streamlit UI Loaded
2025-12-25 10:15:54,780 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:15:54,780 - INFO - src.security - Starting requirement validation process
2025-12-25 10:15:54,780 - INFO - src.security - Requirement validation successful
2025-12-25 10:15:54,780 - INFO - root - Requirement validation successful
2025-12-25 10:15:54,781 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 10:15:54,781 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:15:54,781 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 10:15:54,825 - ERROR - src.ollama_client - Ollama execution failed | Return Code: 1 | Error: Error: unknown flag: --num-predict

2025-12-25 10:15:54,825 - ERROR - src.ollama_client - AI processing error
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed
2025-12-25 10:15:54,826 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 85, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error
2025-12-25 10:15:54,827 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 69, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 85, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 112, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 10:16:10,208 - INFO - root - Streamlit UI Loaded
2025-12-25 10:16:14,354 - INFO - root - Streamlit UI Loaded
2025-12-25 10:16:14,582 - INFO - root - Streamlit UI Loaded
2025-12-25 10:16:14,594 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:16:14,594 - INFO - src.security - Starting requirement validation process
2025-12-25 10:16:14,595 - INFO - src.security - Requirement validation successful
2025-12-25 10:16:14,595 - INFO - root - Requirement validation successful
2025-12-25 10:16:14,597 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 10:16:14,598 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:16:14,598 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 10:21:14,650 - ERROR - src.ollama_client - Ollama process timed out
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 54, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=300  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nLogin using username and password. Lock account after 5 failed attempts.\n\n\nYou are NOT a chatbot.\nDO NOT greet.\nDO NOT explain anything.\nDO NOT add any extra text.\n\nReturn ONLY structured manual QA test cases in this exact format:\n\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nPRE CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\nGenerate:\n- 4 Functional Scenarios\n- 2 Negative Scenarios\n- 1 Boundary Scenario\n- 1 Edge Case Scenario\n- Acceptance Criteria Section\n- Short Test Summary Section\n\nSTRICT RULES:\n- No greetings\n- No emojis\n- No narrative text\n- Only structured test cases\n']' timed out after 300 seconds
2025-12-25 10:21:14,659 - INFO - src.ollama_client - Ollama process timed out
2025-12-25 10:21:14,660 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 54, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=300  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nLogin using username and password. Lock account after 5 failed attempts.\n\n\nYou are NOT a chatbot.\nDO NOT greet.\nDO NOT explain anything.\nDO NOT add any extra text.\n\nReturn ONLY structured manual QA test cases in this exact format:\n\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nPRE CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\nGenerate:\n- 4 Functional Scenarios\n- 2 Negative Scenarios\n- 1 Boundary Scenario\n- 1 Edge Case Scenario\n- Acceptance Criteria Section\n- Short Test Summary Section\n\nSTRICT RULES:\n- No greetings\n- No emojis\n- No narrative text\n- Only structured test cases\n']' timed out after 300 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 80, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out
2025-12-25 10:21:14,666 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 54, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=300  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nLogin using username and password. Lock account after 5 failed attempts.\n\n\nYou are NOT a chatbot.\nDO NOT greet.\nDO NOT explain anything.\nDO NOT add any extra text.\n\nReturn ONLY structured manual QA test cases in this exact format:\n\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nPRE CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\nGenerate:\n- 4 Functional Scenarios\n- 2 Negative Scenarios\n- 1 Boundary Scenario\n- 1 Edge Case Scenario\n- Acceptance Criteria Section\n- Short Test Summary Section\n\nSTRICT RULES:\n- No greetings\n- No emojis\n- No narrative text\n- Only structured test cases\n']' timed out after 300 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 94, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 80, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 102, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 112, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 10:43:41,734 - INFO - root - Streamlit UI Loaded
2025-12-25 10:43:47,059 - INFO - root - Streamlit UI Loaded
2025-12-25 10:43:47,274 - INFO - root - Streamlit UI Loaded
2025-12-25 10:43:47,283 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:43:47,284 - INFO - src.security - Starting requirement validation process
2025-12-25 10:43:47,284 - INFO - src.security - Requirement validation successful
2025-12-25 10:43:47,285 - INFO - root - Requirement validation successful
2025-12-25 10:43:47,287 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 10:43:47,289 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:43:47,290 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 10:46:34,113 - INFO - src.ollama_client - AI response received successfully
2025-12-25 10:46:34,113 - INFO - src.generator - Test case generation completed successfully
2025-12-25 10:46:34,116 - INFO - root - Test Cases delivered successfully to UI
2025-12-25 10:48:14,029 - INFO - root - Streamlit UI Loaded
2025-12-25 10:48:14,225 - INFO - root - Streamlit UI Loaded
2025-12-25 10:48:14,228 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:48:14,229 - INFO - src.security - Starting requirement validation process
2025-12-25 10:48:14,229 - INFO - src.security - Requirement validation successful
2025-12-25 10:48:14,229 - INFO - root - Requirement validation successful
2025-12-25 10:48:14,230 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 10:48:14,231 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:48:14,231 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 10:50:28,486 - INFO - src.ollama_client - AI response received successfully
2025-12-25 10:50:28,487 - INFO - src.generator - Test case generation completed successfully
2025-12-25 10:50:28,489 - INFO - root - Test Cases delivered successfully to UI
2025-12-25 10:53:39,203 - INFO - root - Streamlit UI Loaded
2025-12-25 10:53:46,189 - INFO - root - Streamlit UI Loaded
2025-12-25 10:53:46,417 - INFO - root - Streamlit UI Loaded
2025-12-25 10:53:46,421 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:53:46,421 - INFO - src.security - Starting requirement validation process
2025-12-25 10:53:46,421 - INFO - src.security - Requirement validation successful
2025-12-25 10:53:46,422 - INFO - root - Requirement validation successful
2025-12-25 10:53:46,423 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 10:53:46,423 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:53:46,423 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 10:57:03,173 - INFO - src.ollama_client - AI response received successfully
2025-12-25 10:57:03,173 - INFO - src.generator - Test case generation completed successfully
2025-12-25 10:57:03,176 - INFO - root - Test Cases delivered successfully to UI
2025-12-25 10:57:20,570 - INFO - root - Streamlit UI Loaded
2025-12-25 10:57:26,495 - INFO - root - Streamlit UI Loaded
2025-12-25 10:57:26,721 - INFO - root - Streamlit UI Loaded
2025-12-25 10:57:26,725 - INFO - root - User clicked Generate Test Cases
2025-12-25 10:57:26,726 - INFO - src.security - Starting requirement validation process
2025-12-25 10:57:26,726 - INFO - src.security - Requirement validation successful
2025-12-25 10:57:26,726 - INFO - root - Requirement validation successful
2025-12-25 10:57:26,727 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 10:57:26,728 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 10:57:26,728 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 11:00:40,226 - INFO - src.ollama_client - AI response received successfully
2025-12-25 11:00:40,226 - INFO - src.generator - Test case generation completed successfully
2025-12-25 11:00:40,230 - INFO - root - Test Cases delivered successfully to UI
2025-12-25 11:11:02,223 - INFO - root - Streamlit UI Loaded
2025-12-25 11:11:19,848 - INFO - root - Streamlit UI Loaded
2025-12-25 11:11:20,035 - INFO - root - Streamlit UI Loaded
2025-12-25 11:11:20,040 - INFO - root - User clicked Generate Test Cases
2025-12-25 11:11:20,040 - INFO - src.security - Starting requirement validation process
2025-12-25 11:11:20,041 - INFO - src.security - Requirement validation successful
2025-12-25 11:11:20,041 - INFO - root - Requirement validation successful
2025-12-25 11:11:20,042 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 11:11:20,043 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 11:11:20,043 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 11:12:07,121 - INFO - root - Streamlit UI Loaded
2025-12-25 11:12:10,992 - INFO - root - Streamlit UI Loaded
2025-12-25 11:12:11,213 - INFO - root - Streamlit UI Loaded
2025-12-25 11:12:11,236 - INFO - root - User clicked Generate Test Cases
2025-12-25 11:12:11,237 - INFO - src.security - Starting requirement validation process
2025-12-25 11:12:11,238 - INFO - src.security - Requirement validation successful
2025-12-25 11:12:11,239 - INFO - root - Requirement validation successful
2025-12-25 11:12:11,244 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 11:12:11,249 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 11:12:11,250 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 11:16:20,077 - ERROR - src.ollama_client - Ollama process timed out
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 54, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=300  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nLogin with username and password. Lock after 5 failed attempts.\n\n\nIMPORTANT EXECUTION RULES:\n- You are NOT a chatbot\n- DO NOT greet\n- DO NOT explain anything\n- DO NOT use markdown formatting\n- ONLY plain text output\n- STRICT structure only\n- NO emojis\n- NO extra narrative sentences\n- NO headings other than given\n\n====================================================\nTRACEABILITY MATRIX\n====================================================\nRequirement ID | Scenario Name | Test Case ID | Coverage Status\n\n====================================================\nTEST CASES\n====================================================\n\nSCENARIO CATEGORY: (Functional / Negative / Boundary / Edge)\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nSEVERITY:\nPRE CONDITIONS:\nPOST CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\n\nGENERATE EXACTLY:\n2 Functional Scenarios\n2 Negative Scenarios\n1 Boundary Scenario\n1 Edge Case Scenario\n\n====================================================\nACCEPTANCE CRITERIA\n====================================================\n1)\n2)\n3)\n\n====================================================\nAUTOMATION READY OUTPUT (BDD FORMAT)\n====================================================\nAutomation ID:\nGiven\nWhen\nThen\n\n====================================================\nTEST SUMMARY\n====================================================\nTotal Scenarios:\nTotal Test Cases:\nCoverage Level (High/Medium/Low):\nRisk Level:\nConfidence Level:\n']' timed out after 300 seconds
2025-12-25 11:16:20,080 - INFO - src.ollama_client - Ollama process timed out
2025-12-25 11:16:20,080 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 54, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=300  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nLogin with username and password. Lock after 5 failed attempts.\n\n\nIMPORTANT EXECUTION RULES:\n- You are NOT a chatbot\n- DO NOT greet\n- DO NOT explain anything\n- DO NOT use markdown formatting\n- ONLY plain text output\n- STRICT structure only\n- NO emojis\n- NO extra narrative sentences\n- NO headings other than given\n\n====================================================\nTRACEABILITY MATRIX\n====================================================\nRequirement ID | Scenario Name | Test Case ID | Coverage Status\n\n====================================================\nTEST CASES\n====================================================\n\nSCENARIO CATEGORY: (Functional / Negative / Boundary / Edge)\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nSEVERITY:\nPRE CONDITIONS:\nPOST CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\n\nGENERATE EXACTLY:\n2 Functional Scenarios\n2 Negative Scenarios\n1 Boundary Scenario\n1 Edge Case Scenario\n\n====================================================\nACCEPTANCE CRITERIA\n====================================================\n1)\n2)\n3)\n\n====================================================\nAUTOMATION READY OUTPUT (BDD FORMAT)\n====================================================\nAutomation ID:\nGiven\nWhen\nThen\n\n====================================================\nTEST SUMMARY\n====================================================\nTotal Scenarios:\nTotal Test Cases:\nCoverage Level (High/Medium/Low):\nRisk Level:\nConfidence Level:\n']' timed out after 300 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 109, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 80, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out
2025-12-25 11:17:11,292 - ERROR - src.ollama_client - Ollama process timed out
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 54, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=300  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nLogin with username and password. Lock after 5 failed attempts.\n\n\nIMPORTANT EXECUTION RULES:\n- You are NOT a chatbot\n- DO NOT greet\n- DO NOT explain anything\n- DO NOT use markdown formatting\n- ONLY plain text output\n- STRICT structure only\n- NO emojis\n- NO extra narrative sentences\n- NO headings other than given\n\n====================================================\nTRACEABILITY MATRIX\n====================================================\nRequirement ID | Scenario Name | Test Case ID | Coverage Status\n\n====================================================\nTEST CASES\n====================================================\n\nSCENARIO CATEGORY: (Functional / Negative / Boundary / Edge)\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nSEVERITY:\nPRE CONDITIONS:\nPOST CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\n\nGENERATE EXACTLY:\n2 Functional Scenarios\n2 Negative Scenarios\n1 Boundary Scenario\n1 Edge Case Scenario\n\n====================================================\nACCEPTANCE CRITERIA\n====================================================\n1)\n2)\n3)\n\n====================================================\nAUTOMATION READY OUTPUT (BDD FORMAT)\n====================================================\nAutomation ID:\nGiven\nWhen\nThen\nContinue all required for all steps...\n====================================================\nTEST SUMMARY\n====================================================\nTotal Scenarios:\nTotal Test Cases:\nCoverage Level (High/Medium/Low):\nRisk Level:\nConfidence Level:\n']' timed out after 300 seconds
2025-12-25 11:17:11,295 - INFO - src.ollama_client - Ollama process timed out
2025-12-25 11:17:11,295 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 54, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=300  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nLogin with username and password. Lock after 5 failed attempts.\n\n\nIMPORTANT EXECUTION RULES:\n- You are NOT a chatbot\n- DO NOT greet\n- DO NOT explain anything\n- DO NOT use markdown formatting\n- ONLY plain text output\n- STRICT structure only\n- NO emojis\n- NO extra narrative sentences\n- NO headings other than given\n\n====================================================\nTRACEABILITY MATRIX\n====================================================\nRequirement ID | Scenario Name | Test Case ID | Coverage Status\n\n====================================================\nTEST CASES\n====================================================\n\nSCENARIO CATEGORY: (Functional / Negative / Boundary / Edge)\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nSEVERITY:\nPRE CONDITIONS:\nPOST CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\n\nGENERATE EXACTLY:\n2 Functional Scenarios\n2 Negative Scenarios\n1 Boundary Scenario\n1 Edge Case Scenario\n\n====================================================\nACCEPTANCE CRITERIA\n====================================================\n1)\n2)\n3)\n\n====================================================\nAUTOMATION READY OUTPUT (BDD FORMAT)\n====================================================\nAutomation ID:\nGiven\nWhen\nThen\nContinue all required for all steps...\n====================================================\nTEST SUMMARY\n====================================================\nTotal Scenarios:\nTotal Test Cases:\nCoverage Level (High/Medium/Low):\nRisk Level:\nConfidence Level:\n']' timed out after 300 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 109, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 80, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out
2025-12-25 11:17:11,297 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 54, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=300  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nLogin with username and password. Lock after 5 failed attempts.\n\n\nIMPORTANT EXECUTION RULES:\n- You are NOT a chatbot\n- DO NOT greet\n- DO NOT explain anything\n- DO NOT use markdown formatting\n- ONLY plain text output\n- STRICT structure only\n- NO emojis\n- NO extra narrative sentences\n- NO headings other than given\n\n====================================================\nTRACEABILITY MATRIX\n====================================================\nRequirement ID | Scenario Name | Test Case ID | Coverage Status\n\n====================================================\nTEST CASES\n====================================================\n\nSCENARIO CATEGORY: (Functional / Negative / Boundary / Edge)\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nSEVERITY:\nPRE CONDITIONS:\nPOST CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\n\nGENERATE EXACTLY:\n2 Functional Scenarios\n2 Negative Scenarios\n1 Boundary Scenario\n1 Edge Case Scenario\n\n====================================================\nACCEPTANCE CRITERIA\n====================================================\n1)\n2)\n3)\n\n====================================================\nAUTOMATION READY OUTPUT (BDD FORMAT)\n====================================================\nAutomation ID:\nGiven\nWhen\nThen\nContinue all required for all steps...\n====================================================\nTEST SUMMARY\n====================================================\nTotal Scenarios:\nTotal Test Cases:\nCoverage Level (High/Medium/Low):\nRisk Level:\nConfidence Level:\n']' timed out after 300 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 109, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 80, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 91, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 127, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 11:18:56,376 - INFO - root - Streamlit UI Loaded
2025-12-25 11:19:11,451 - INFO - root - Streamlit UI Loaded
2025-12-25 11:19:11,663 - INFO - root - Streamlit UI Loaded
2025-12-25 11:19:11,680 - INFO - root - User clicked Generate Test Cases
2025-12-25 11:19:11,681 - INFO - src.security - Starting requirement validation process
2025-12-25 11:19:11,682 - INFO - src.security - Requirement validation successful
2025-12-25 11:19:11,683 - INFO - root - Requirement validation successful
2025-12-25 11:19:11,686 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 11:19:11,688 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 11:19:11,692 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 11:24:11,731 - ERROR - src.ollama_client - Ollama process timed out
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 54, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=300  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nLogin with username and password. Lock after 5 failed attempts.\n\n\nIMPORTANT EXECUTION RULES:\n- You are NOT a chatbot\n- DO NOT greet\n- DO NOT explain anything\n- DO NOT use markdown formatting\n- ONLY plain text output\n- STRICT structure only\n- NO emojis\n- NO extra narrative sentences\n- NO headings other than given\n\n====================================================\nTRACEABILITY MATRIX\n====================================================\nRequirement ID | Scenario Name | Test Case ID | Coverage Status\n\n====================================================\nTEST CASES\n====================================================\n\nSCENARIO CATEGORY: (Functional / Negative / Boundary / Edge)\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nSEVERITY:\nPRE CONDITIONS:\nPOST CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\n\nGENERATE EXACTLY:\n2 Functional Scenarios\n2 Negative Scenarios\n1 Boundary Scenario\n1 Edge Case Scenario\n\n====================================================\nACCEPTANCE CRITERIA\n====================================================\n1)\n2)\n3)\n\n====================================================\nTEST SUMMARY\n====================================================\nTotal Scenarios:\nTotal Test Cases:\nCoverage Level (High/Medium/Low):\nRisk Level:\nConfidence Level:\n']' timed out after 300 seconds
2025-12-25 11:24:11,734 - INFO - src.ollama_client - Ollama process timed out
2025-12-25 11:24:11,735 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 54, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=300  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nLogin with username and password. Lock after 5 failed attempts.\n\n\nIMPORTANT EXECUTION RULES:\n- You are NOT a chatbot\n- DO NOT greet\n- DO NOT explain anything\n- DO NOT use markdown formatting\n- ONLY plain text output\n- STRICT structure only\n- NO emojis\n- NO extra narrative sentences\n- NO headings other than given\n\n====================================================\nTRACEABILITY MATRIX\n====================================================\nRequirement ID | Scenario Name | Test Case ID | Coverage Status\n\n====================================================\nTEST CASES\n====================================================\n\nSCENARIO CATEGORY: (Functional / Negative / Boundary / Edge)\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nSEVERITY:\nPRE CONDITIONS:\nPOST CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\n\nGENERATE EXACTLY:\n2 Functional Scenarios\n2 Negative Scenarios\n1 Boundary Scenario\n1 Edge Case Scenario\n\n====================================================\nACCEPTANCE CRITERIA\n====================================================\n1)\n2)\n3)\n\n====================================================\nTEST SUMMARY\n====================================================\nTotal Scenarios:\nTotal Test Cases:\nCoverage Level (High/Medium/Low):\nRisk Level:\nConfidence Level:\n']' timed out after 300 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 101, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 80, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out
2025-12-25 11:24:11,740 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 54, in generate_ai_response
    result = subprocess.run(
        cmd,
    ...<2 lines>...
        timeout=300  # Prevent infinite hang
    )
  File "C:\Python313\Lib\subprocess.py", line 556, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1222, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
                     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\subprocess.py", line 1646, in _communicate
    raise TimeoutExpired(self.args, orig_timeout)
subprocess.TimeoutExpired: Command '['ollama', 'run', 'llama3:8b', '\nRequirement:\nLogin with username and password. Lock after 5 failed attempts.\n\n\nIMPORTANT EXECUTION RULES:\n- You are NOT a chatbot\n- DO NOT greet\n- DO NOT explain anything\n- DO NOT use markdown formatting\n- ONLY plain text output\n- STRICT structure only\n- NO emojis\n- NO extra narrative sentences\n- NO headings other than given\n\n====================================================\nTRACEABILITY MATRIX\n====================================================\nRequirement ID | Scenario Name | Test Case ID | Coverage Status\n\n====================================================\nTEST CASES\n====================================================\n\nSCENARIO CATEGORY: (Functional / Negative / Boundary / Edge)\nSCENARIO NAME:\nOBJECTIVE:\nTEST CASE ID:\nPRIORITY:\nSEVERITY:\nPRE CONDITIONS:\nPOST CONDITIONS:\n\nTEST STEPS WITH EXPECTED RESULTS:\n1) Step\n   Expected Result:\n2) Step\n   Expected Result:\n3) Step\n   Expected Result:\n\nEXPECTED FINAL OUTCOME:\n\n------------------------------------\n\nGENERATE EXACTLY:\n2 Functional Scenarios\n2 Negative Scenarios\n1 Boundary Scenario\n1 Edge Case Scenario\n\n====================================================\nACCEPTANCE CRITERIA\n====================================================\n1)\n2)\n3)\n\n====================================================\nTEST SUMMARY\n====================================================\nTotal Scenarios:\nTotal Test Cases:\nCoverage Level (High/Medium/Low):\nRisk Level:\nConfidence Level:\n']' timed out after 300 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 101, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 80, in generate_ai_response
    raise RuntimeError("Ollama request timed out")
RuntimeError: Ollama request timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 91, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 119, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 11:24:38,129 - INFO - root - Streamlit UI Loaded
2025-12-25 11:24:38,437 - INFO - root - Streamlit UI Loaded
2025-12-25 11:24:43,760 - INFO - root - Streamlit UI Loaded
2025-12-25 11:24:43,993 - INFO - root - Streamlit UI Loaded
2025-12-25 11:24:43,996 - INFO - root - User clicked Generate Test Cases
2025-12-25 11:24:43,996 - INFO - src.security - Starting requirement validation process
2025-12-25 11:24:43,997 - INFO - src.security - Requirement validation successful
2025-12-25 11:24:43,997 - INFO - root - Requirement validation successful
2025-12-25 11:24:43,998 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 11:24:43,998 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 11:24:43,998 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 11:28:04,488 - INFO - src.ollama_client - AI response received successfully
2025-12-25 11:28:04,488 - INFO - src.generator - Test case generation completed successfully
2025-12-25 11:28:04,493 - INFO - root - Test Cases delivered successfully to UI
2025-12-25 11:51:21,602 - INFO - root - Streamlit UI Loaded
2025-12-25 11:51:29,941 - INFO - root - Streamlit UI Loaded
2025-12-25 11:51:30,162 - INFO - root - Streamlit UI Loaded
2025-12-25 11:51:30,166 - INFO - root - User clicked Generate Test Cases
2025-12-25 11:51:30,167 - INFO - src.security - Starting requirement validation process
2025-12-25 11:51:30,167 - INFO - src.security - Requirement validation successful
2025-12-25 11:51:30,168 - INFO - root - Requirement validation successful
2025-12-25 11:51:30,169 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 11:51:30,170 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 11:51:30,170 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 11:51:30,222 - ERROR - src.ollama_client - Ollama execution failed | Return Code: 1 | Error: Error: unknown flag: --num-predict

2025-12-25 11:51:30,222 - ERROR - src.ollama_client - AI processing error
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 75, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed
2025-12-25 11:51:30,223 - ERROR - src.generator - Test case generation failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 75, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 101, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 90, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error
2025-12-25 11:51:30,225 - ERROR - root - Streamlit Execution Failed
Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 75, in generate_ai_response
    raise RuntimeError("Ollama execution failed")
RuntimeError: Ollama execution failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 101, in generate_test_cases
    response = generate_ai_response(prompt)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\ollama_client.py", line 90, in generate_ai_response
    raise RuntimeError("AI processing error") from e
RuntimeError: AI processing error

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\app_streamlit.py", line 91, in <module>
    result = generate_test_cases(requirement)
  File "C:\Users\HP\OneDrive\Desktop\ai-test-generator\src\generator.py", line 119, in generate_test_cases
    raise RuntimeError("Test case generation failed") from e
RuntimeError: Test case generation failed
2025-12-25 11:51:46,320 - INFO - root - Streamlit UI Loaded
2025-12-25 11:51:50,800 - INFO - root - Streamlit UI Loaded
2025-12-25 11:51:51,015 - INFO - root - Streamlit UI Loaded
2025-12-25 11:51:51,023 - INFO - root - User clicked Generate Test Cases
2025-12-25 11:51:51,024 - INFO - src.security - Starting requirement validation process
2025-12-25 11:51:51,026 - INFO - src.security - Requirement validation successful
2025-12-25 11:51:51,027 - INFO - root - Requirement validation successful
2025-12-25 11:51:51,029 - INFO - src.generator - Preparing test case generation request | Mode: fast
2025-12-25 11:51:51,031 - INFO - src.ollama_client - Preparing AI request to Ollama model: llama3:8b
2025-12-25 11:51:51,032 - INFO - src.ollama_client - Sending request to Ollama engine
2025-12-25 11:55:09,116 - INFO - src.ollama_client - AI response received successfully
2025-12-25 11:55:09,117 - INFO - src.generator - Test case generation completed successfully
2025-12-25 11:55:09,121 - INFO - root - Test Cases delivered successfully to UI
